---
date: "06 2006"
external_link: ""
image:
  caption:
  focal_point: Smart
summary:
tags:
- Statistical learning
- verb-argument structure overgeneralizations
- entrenchment
- preemption
title: Avoiding overgeneralizations
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""
---

This strand of work uses artificial language learning methods to explore different theories about how we children lean to avoid overgeneralization errors: When children learn their mother tongue, they sometimes make errors such as “he giggled me!”, “she fell me over!” or “carry me teddy”. Learning to avoid these mistakes in English (and many other languages of the world) involves learning that some verbs can appear in a two-person sentence (e.g., “Bob tickled Wendy”), while other verbs cannot (e.g., “Bob giggled Wendy”). Evidence suggests that children get little direct correction for these types of errors, yet they eventually stop making them. What discourages children them from producing such incorrect sentences in the long term? A series of ongoing experiments funded by the ERC (awarded to Professor Ben Ambridge) investigate whether frequency may be the key: for example, does repeated exposure to the verb “giggle” within sentences that mean “causing someone to laugh” (e.g., “The funny clown/the teacher made the audience/the students giggle”) gradually discourage speakers from using it in a two-person sentence?
